{"title":"Extracting date and time from camera trap photos using R and tesseract","markdown":{"yaml":{"title":"Extracting date and time from camera trap photos using R and tesseract","author":"","date":"2021-03-29","slug":[],"categories":["code","R"],"tags":["R","code","camera trap","OCR"],"description":"","featured":"","featuredalt":"","featuredpath":"","linktitle":""},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nRecently I came across an interesting issue. My University runs a field station at a rural property called Newholme where a grid of camera traps was set up a few years back. Cameras are solar-powered and form a mesh network where photos are relayed from one camera to another and finally to the base station located at the station building. Unfortunately, due to a firmware bug, for the first few years, the EXIF data was wiped from each photo as it was relayed so we could only get the date and time when the file was saved on the base station, which could be minutes or days after it was taken. Original metadata was embedded in the picture itself as text, like this:\n\n![sample image](pic.jpg)\n\nThe manufacturer (Buckeye) eventually fixed this issue and after a firmware update photos now have metadata embedded in EXIF fields. However, to use all the data from before the update, we needed to assign date and time to each picture. It was done manually for some small subset but it is a tedious process prone to errors, so I decided to try and automate it.\n\n# OCR and the tesseract package\n\nI have used OCR (optical character recognition) software before and I thought there is probably an R package that can read the text and sure enough - a powerful, open-source OCR engine tesseract is availabe in R through a package [tesseract](https://cran.r-project.org/web/packages/tesseract/index.html).\n\nWe will need a couple of libraries first:\n\n```         \n\nlibrary(magick)\nlibrary(tesseract)\nlibrary(tidyverse)\n```\n\nTo help tesseract read our data, we can clip interesting parts from the photo. In this case, I want to read the date and time so we can clip that part of the image. We can use code:\n\n```         \n# import photo\nimage_read(photo1)%>%\n\n# crop date field\nimage_crop(\"68x12+0+468\")\n```\n\nWhere first two values (68x12) represent the size of the cropped field, and the second two (+0+468) represent the position of the top left corner of that field. As we can see, that gives us desired part of the photo:\n\n![cropped date field](date1.png)\n\nTesseract is not dealing well with inverted images so we need to do some more preprocessing before we can run the OCR engine. We will invert the picture, then we will enlarge it 10 times (photo is only 640x480 px and the original digits are only 9 pixels high), increase brightness to get rid of the noise and increase contrast to make it easier to read for our engine. This required a little bit of trial and error to get it right, but here is the result:\n\n```         \n\nimage_read(photo1)%>%\nimage_negate()%>%\nimage_crop(\"68x12+0+468\")%>%\nimage_scale(\"680x120\") %>%\nimage_modulate(brightness = 120)%>%\nimage_contrast(sharpen = 100)\n```\n\n![preprocessed date field](date2.jpg)\n\nThis is already looking better. The narrow strip on the right is there because the actual width of the field changes depending on the number of digits and otherwise would clip the last digit on some long dates. We will deal with it later.\n\nNow we can run the OCR engine and see what we get.\n\n```         \n\n#import and preprocess the image\ndate <- image_read(photo1)%>%\nimage_negate()%>%\nimage_crop(\"68x12+0+468\")%>%\nimage_scale(\"680x120\") %>%\nimage_modulate(brightness = 120)%>%\nimage_contrast(sharpen = 100)\n\n#read the text\nocr(date)\n```\n\nWhich gives us \\>\"3/13/2415 \\|\\n\"\n\nAs you can see, tesseract wrongly identified 0 as 4. For other photos, the results varied from correct or almost correct to a random sequence of letters and digits that didn't make any sense. The \"pixelated\" font that the camera is using is making it hard for the engine to read some digits correctly. We need to dig deeper and make use of the tesseract training mechanism.\n\nTo do that we can prepare a training set consisting of our preprocessed crops, load them into a program, generate a box for each digit and then assign it a correct value. We can generate our training dataset using a simple loop to crop date and time fields from each photo, preprocess it and save each one as a new file:\n\n```         \n\n#create a list of training files\nfiles <- list.files(path=\"./Datasets/02a\", full.names=TRUE)\n\n#using the list of images \nfor (row in 1:NROW(files)) {\n  \n  #display iteration to see progress\n  print(row)\n  \n  # read the i-th image and invert it\n  pic <- image_negate(image_read(files[row]))\n  \n  # extract parts of the image to read\n\n  \n  date <- image_crop(pic, \"68x12+0+468\")%>%\n    image_scale(\"680x120\") %>%\n    image_modulate(brightness = 140)%>%\n    image_contrast(sharpen = 100)%>%\n    image_median(radius = 5)%>%\n    \n    \n   time <- image_crop(pic, \"50x12+296+468\")%>%\n    image_scale(\"500x120\") %>%\n    image_modulate(brightness = 140)%>%\n    image_contrast(sharpen = 100)%>%\n    image_median(radius = 5)%>%\n    image_quantize(max=2)\n  \n\n  \n #write output for training\n  \n  newfile <- file.path(paste0(\"./Datasets/Training/date_\", row, \".jpg\"))\n  \n  image_write(date, newfile, quality=100)\n  \n  newfile <- file.path(paste0(\"./Datasets/Training/time_\", row, \".jpg\"))\n  \n  image_write(time, newfile, quality=100)\n  \n}\n```\n\nNow we can move to software for tesseract training (it can be done from the terminal as well, but since I had no prior experience with tesseract, I found this software to be easier to use). We need to download [jTessBoxEditor](http://vietocr.sourceforge.net/training.html) and follow the instructions. Combine our training files into a multi-page TIFF (Tools-\\>Merge TIFF), then we go to the *Trainer* tab, select our input file (the newly generated TIFF) and select *Generate Box File* from the drop-down list. It will then generate a box around each digit and assign values, best it can. We can now move to the *Box Editor* tab, load our newly generated training set and go through the boxes and edit them to fix values that were wrongly assigned:\n\n![training boxes](box.png)\n\nI did that for a sample of 142 dates and times (around 2000 characters in total). We then save our training data, go back to the trainer tab and use it for the actual training, selecting functions: *Train with existing box*, then *Shape Clustering* and finally *Dictionary*. This results in a new \"language\" (in our case simply a font) that can be used as a parameter in the OCR engine.\n\nSo we need to set up this new engine by assigning a language (I called it \"buc\"). We can further help tesseract by limiting the symbols it will look for to just digits, \"/\" and \":\"\" since there are no other characters in the date and time fields:\n\n```         \n\nnumbers <- tesseract(\"buc\", datapath = \"./Datasets/train\", options = list(tessedit_char_whitelist = \"0123456789:/\"))\n\nocr(test, engine = numbers)\n```\n\nRunning this on our dataset gives much better results, but for whatever reason, the engine is still struggling with some digits, particularly 6, 8 and 0. Even though these are generated by the camera trap and are identical, the same digit is sometimes read correctly, and sometimes not, with 8 being misread every single time as either 6 or 0. I spent hours trying to tweak the training data and even though it met all the guidelines (more than 20 instances of every digit etc.) I could never get it to work. Not only that, I couldn't find a good reason for it not working **sometimes** but then working well **on the same digits**, just in different order.\n\nI then decided to see if the camera is using an existing font as jTessBoxEditor has an option to generate training files using a known font and it was recommended in the tesseract manual as the best way to train the data. I took a sample date and uploaded it to online font recognition websites like *WhatTheFont!* and *What Font Is*, found some similar looking (but not identical) fonts and tried that, but it didn't make a huge difference and in some cases made it worse. So finally I came up with an idea to create a custom font that would be as close as possible to the one used by Buckeye and use that for training. I found a very helpful website [calligraphr](calligraphr.com) which allows you to create a font from your handwriting. You select the symbols you want to use, then you download an automatically generated template to fill with your handwriting (in my case - paste in preprocessed digits) and upload it back to the website which converts it into a font:\n\n![online font generator - input](cali.png)\n\nYou then get a custom generated font that you can install on your computer (I recommend using \"install for all users\", you will also need to restart jTessBoxEditor). You then need to create a text file with a sample text - in my case, it was just 0123456789/: repeated 20 times - and use the *TIFF/Box Generator* tab to generate a new training file:\n\n![boxes generated with the custom font](box2.png)\n\nThen you need to repeat the training process. **Do not** generate new boxes (boxes with correct characters are generated by TIFF/Box Generator), go straight to training using *Train with existing box* , *Shape Clustering* and *Dictionary* and the result is your new \"language\" that can be then used as a parameter in the tesseract() function. Great thing is that it takes a couple of minutes and not hours like in the previous method.\n\nNow, using the custom engine, I was getting **100% accuracy** (I checked it on 2000 sample photos and there was not a single error) - finally! We can now move to a final code to run through the dataset, extract dates and time and write them into a .csv file. Our photos are already arranged into folders by camera and species so we can use the file path to easily extract these values afterwards.\n\nThe last thing to mention is that, like in my first example, there are some narrow strips on some crops that are read as digits. Luckily, they are always separated by a space, so we can add a line to remove that space and everything after it from our read values. We also need to get rid of the new line symbol \\[\\n\\] at the end of each record which tesseract adds automatically - we will do with another line of code.\n\n# Final code\n\nThis is what the final code looks like:\n\n```         \n### Extracting metadata from camera trap images\n\n# load required libraries\n\nlibrary(magick)\nlibrary(tesseract)\n\n\n# building a tesseract engine\n# using a pre-trained dataset \"buckeye\" (stored in the folder \"font\")\n\nbuckeye <- tesseract(\"buckeye\", datapath = \"./font\")\n\n\n\n# generate a list of files - only .jpg as another format will cause error\n\nfiles <- list.files(path=\"./Data\", full.names=TRUE, recursive = TRUE, pattern = \"jpg\")\n\n\n\n#loop for extracting date and time\n\n#initialise variables\n\nno <- NA\nt <- NA\nd <- NA\n\n\n#starting from the first file and moving through the list of images \n\nfor (row in 1:NROW(files)) {\n  \n  \n  #display progress\n  \n  message(paste0(\"analysing photo \", row, \" out of \", NROW(files)))\n  \n\n  ### actual OCR routine\n  # read the n-th image and invert it\n  \n  pic <- image_negate(image_read(files[row]))\n  \n  \n  # extract parts of the image to read and preprocess for OCR\n  \n   date <- image_crop(pic, \"68x12+0+468\")%>%\n    image_scale(\"680x120\") %>%\n    image_modulate(brightness = 120)%>%\n    image_contrast(sharpen = 100)\n  \n  time <- image_crop(pic, \"50x12+296+468\")%>%\n    image_scale(\"500x120\") %>%\n    image_modulate(brightness = 120)%>%\n    image_contrast(sharpen = 100)\n  \n \n  \n  #read them and write into n-th row of respective variable\n  \n  no[row] <- row\n  d[row] <- ocr(date, engine = buckeye)\n  t[row] <- ocr(time, engine = buckeye)\n  \n  # move to the next file\n}\n\n# cleaning up the data\n# remove artefacts and page breaks\n\nt <- sub(\" .*\", \"\", t)\nt <- sub(\"[\\n]\", \"\", t)\n\nd <- sub(\" .*\", \"\\n\", d)\nd <- sub(\"[\\n]\", \"\", d)\n\n\n#create a dataframe with the read values\n\noutput <- data.frame(no = no,\n                     filename = files,\n                     date <- d,\n                     time <- t)\n\n#write a .csv file\n\nwrite.csv(output, \"extracted_date_and_time.csv\")\n\n```\n\nYou can now put all the photos to be tagged into the Data folder and execute the code. Depending on how many photos you have, how large your photos are and how fast your computer is it can take from minutes to hours. I was getting around 200 photos per minute on my 7 years old laptop, while my supervisor was going through more than 1000 photos per minute on his much newer and faster Mac. Still, I recommend running it in smaller chunks, that way in case of an error you are not losing 10h of processing.\n\nI intentionally didn't want to do any editing of the existing photos (disk space, maintaining 2 copies etc) and instead decided to save the results as a separate .csv file. In case you want to rename your photos and write this metadata into the picture itself, I recommend you read a very informative [post](https://johnpvanek.weebly.com/intothephrag/archives/07-2019) by John Vanek where he details how to manipulate EXIF data. Luckily, he was able to use the default tesseract engine with great success and if your camera has better resolution and uses a more standard font, you might not need to worry about training the engine at all.\n","srcMarkdownNoYaml":"\n\n# Background\n\nRecently I came across an interesting issue. My University runs a field station at a rural property called Newholme where a grid of camera traps was set up a few years back. Cameras are solar-powered and form a mesh network where photos are relayed from one camera to another and finally to the base station located at the station building. Unfortunately, due to a firmware bug, for the first few years, the EXIF data was wiped from each photo as it was relayed so we could only get the date and time when the file was saved on the base station, which could be minutes or days after it was taken. Original metadata was embedded in the picture itself as text, like this:\n\n![sample image](pic.jpg)\n\nThe manufacturer (Buckeye) eventually fixed this issue and after a firmware update photos now have metadata embedded in EXIF fields. However, to use all the data from before the update, we needed to assign date and time to each picture. It was done manually for some small subset but it is a tedious process prone to errors, so I decided to try and automate it.\n\n# OCR and the tesseract package\n\nI have used OCR (optical character recognition) software before and I thought there is probably an R package that can read the text and sure enough - a powerful, open-source OCR engine tesseract is availabe in R through a package [tesseract](https://cran.r-project.org/web/packages/tesseract/index.html).\n\nWe will need a couple of libraries first:\n\n```         \n\nlibrary(magick)\nlibrary(tesseract)\nlibrary(tidyverse)\n```\n\nTo help tesseract read our data, we can clip interesting parts from the photo. In this case, I want to read the date and time so we can clip that part of the image. We can use code:\n\n```         \n# import photo\nimage_read(photo1)%>%\n\n# crop date field\nimage_crop(\"68x12+0+468\")\n```\n\nWhere first two values (68x12) represent the size of the cropped field, and the second two (+0+468) represent the position of the top left corner of that field. As we can see, that gives us desired part of the photo:\n\n![cropped date field](date1.png)\n\nTesseract is not dealing well with inverted images so we need to do some more preprocessing before we can run the OCR engine. We will invert the picture, then we will enlarge it 10 times (photo is only 640x480 px and the original digits are only 9 pixels high), increase brightness to get rid of the noise and increase contrast to make it easier to read for our engine. This required a little bit of trial and error to get it right, but here is the result:\n\n```         \n\nimage_read(photo1)%>%\nimage_negate()%>%\nimage_crop(\"68x12+0+468\")%>%\nimage_scale(\"680x120\") %>%\nimage_modulate(brightness = 120)%>%\nimage_contrast(sharpen = 100)\n```\n\n![preprocessed date field](date2.jpg)\n\nThis is already looking better. The narrow strip on the right is there because the actual width of the field changes depending on the number of digits and otherwise would clip the last digit on some long dates. We will deal with it later.\n\nNow we can run the OCR engine and see what we get.\n\n```         \n\n#import and preprocess the image\ndate <- image_read(photo1)%>%\nimage_negate()%>%\nimage_crop(\"68x12+0+468\")%>%\nimage_scale(\"680x120\") %>%\nimage_modulate(brightness = 120)%>%\nimage_contrast(sharpen = 100)\n\n#read the text\nocr(date)\n```\n\nWhich gives us \\>\"3/13/2415 \\|\\n\"\n\nAs you can see, tesseract wrongly identified 0 as 4. For other photos, the results varied from correct or almost correct to a random sequence of letters and digits that didn't make any sense. The \"pixelated\" font that the camera is using is making it hard for the engine to read some digits correctly. We need to dig deeper and make use of the tesseract training mechanism.\n\nTo do that we can prepare a training set consisting of our preprocessed crops, load them into a program, generate a box for each digit and then assign it a correct value. We can generate our training dataset using a simple loop to crop date and time fields from each photo, preprocess it and save each one as a new file:\n\n```         \n\n#create a list of training files\nfiles <- list.files(path=\"./Datasets/02a\", full.names=TRUE)\n\n#using the list of images \nfor (row in 1:NROW(files)) {\n  \n  #display iteration to see progress\n  print(row)\n  \n  # read the i-th image and invert it\n  pic <- image_negate(image_read(files[row]))\n  \n  # extract parts of the image to read\n\n  \n  date <- image_crop(pic, \"68x12+0+468\")%>%\n    image_scale(\"680x120\") %>%\n    image_modulate(brightness = 140)%>%\n    image_contrast(sharpen = 100)%>%\n    image_median(radius = 5)%>%\n    \n    \n   time <- image_crop(pic, \"50x12+296+468\")%>%\n    image_scale(\"500x120\") %>%\n    image_modulate(brightness = 140)%>%\n    image_contrast(sharpen = 100)%>%\n    image_median(radius = 5)%>%\n    image_quantize(max=2)\n  \n\n  \n #write output for training\n  \n  newfile <- file.path(paste0(\"./Datasets/Training/date_\", row, \".jpg\"))\n  \n  image_write(date, newfile, quality=100)\n  \n  newfile <- file.path(paste0(\"./Datasets/Training/time_\", row, \".jpg\"))\n  \n  image_write(time, newfile, quality=100)\n  \n}\n```\n\nNow we can move to software for tesseract training (it can be done from the terminal as well, but since I had no prior experience with tesseract, I found this software to be easier to use). We need to download [jTessBoxEditor](http://vietocr.sourceforge.net/training.html) and follow the instructions. Combine our training files into a multi-page TIFF (Tools-\\>Merge TIFF), then we go to the *Trainer* tab, select our input file (the newly generated TIFF) and select *Generate Box File* from the drop-down list. It will then generate a box around each digit and assign values, best it can. We can now move to the *Box Editor* tab, load our newly generated training set and go through the boxes and edit them to fix values that were wrongly assigned:\n\n![training boxes](box.png)\n\nI did that for a sample of 142 dates and times (around 2000 characters in total). We then save our training data, go back to the trainer tab and use it for the actual training, selecting functions: *Train with existing box*, then *Shape Clustering* and finally *Dictionary*. This results in a new \"language\" (in our case simply a font) that can be used as a parameter in the OCR engine.\n\nSo we need to set up this new engine by assigning a language (I called it \"buc\"). We can further help tesseract by limiting the symbols it will look for to just digits, \"/\" and \":\"\" since there are no other characters in the date and time fields:\n\n```         \n\nnumbers <- tesseract(\"buc\", datapath = \"./Datasets/train\", options = list(tessedit_char_whitelist = \"0123456789:/\"))\n\nocr(test, engine = numbers)\n```\n\nRunning this on our dataset gives much better results, but for whatever reason, the engine is still struggling with some digits, particularly 6, 8 and 0. Even though these are generated by the camera trap and are identical, the same digit is sometimes read correctly, and sometimes not, with 8 being misread every single time as either 6 or 0. I spent hours trying to tweak the training data and even though it met all the guidelines (more than 20 instances of every digit etc.) I could never get it to work. Not only that, I couldn't find a good reason for it not working **sometimes** but then working well **on the same digits**, just in different order.\n\nI then decided to see if the camera is using an existing font as jTessBoxEditor has an option to generate training files using a known font and it was recommended in the tesseract manual as the best way to train the data. I took a sample date and uploaded it to online font recognition websites like *WhatTheFont!* and *What Font Is*, found some similar looking (but not identical) fonts and tried that, but it didn't make a huge difference and in some cases made it worse. So finally I came up with an idea to create a custom font that would be as close as possible to the one used by Buckeye and use that for training. I found a very helpful website [calligraphr](calligraphr.com) which allows you to create a font from your handwriting. You select the symbols you want to use, then you download an automatically generated template to fill with your handwriting (in my case - paste in preprocessed digits) and upload it back to the website which converts it into a font:\n\n![online font generator - input](cali.png)\n\nYou then get a custom generated font that you can install on your computer (I recommend using \"install for all users\", you will also need to restart jTessBoxEditor). You then need to create a text file with a sample text - in my case, it was just 0123456789/: repeated 20 times - and use the *TIFF/Box Generator* tab to generate a new training file:\n\n![boxes generated with the custom font](box2.png)\n\nThen you need to repeat the training process. **Do not** generate new boxes (boxes with correct characters are generated by TIFF/Box Generator), go straight to training using *Train with existing box* , *Shape Clustering* and *Dictionary* and the result is your new \"language\" that can be then used as a parameter in the tesseract() function. Great thing is that it takes a couple of minutes and not hours like in the previous method.\n\nNow, using the custom engine, I was getting **100% accuracy** (I checked it on 2000 sample photos and there was not a single error) - finally! We can now move to a final code to run through the dataset, extract dates and time and write them into a .csv file. Our photos are already arranged into folders by camera and species so we can use the file path to easily extract these values afterwards.\n\nThe last thing to mention is that, like in my first example, there are some narrow strips on some crops that are read as digits. Luckily, they are always separated by a space, so we can add a line to remove that space and everything after it from our read values. We also need to get rid of the new line symbol \\[\\n\\] at the end of each record which tesseract adds automatically - we will do with another line of code.\n\n# Final code\n\nThis is what the final code looks like:\n\n```         \n### Extracting metadata from camera trap images\n\n# load required libraries\n\nlibrary(magick)\nlibrary(tesseract)\n\n\n# building a tesseract engine\n# using a pre-trained dataset \"buckeye\" (stored in the folder \"font\")\n\nbuckeye <- tesseract(\"buckeye\", datapath = \"./font\")\n\n\n\n# generate a list of files - only .jpg as another format will cause error\n\nfiles <- list.files(path=\"./Data\", full.names=TRUE, recursive = TRUE, pattern = \"jpg\")\n\n\n\n#loop for extracting date and time\n\n#initialise variables\n\nno <- NA\nt <- NA\nd <- NA\n\n\n#starting from the first file and moving through the list of images \n\nfor (row in 1:NROW(files)) {\n  \n  \n  #display progress\n  \n  message(paste0(\"analysing photo \", row, \" out of \", NROW(files)))\n  \n\n  ### actual OCR routine\n  # read the n-th image and invert it\n  \n  pic <- image_negate(image_read(files[row]))\n  \n  \n  # extract parts of the image to read and preprocess for OCR\n  \n   date <- image_crop(pic, \"68x12+0+468\")%>%\n    image_scale(\"680x120\") %>%\n    image_modulate(brightness = 120)%>%\n    image_contrast(sharpen = 100)\n  \n  time <- image_crop(pic, \"50x12+296+468\")%>%\n    image_scale(\"500x120\") %>%\n    image_modulate(brightness = 120)%>%\n    image_contrast(sharpen = 100)\n  \n \n  \n  #read them and write into n-th row of respective variable\n  \n  no[row] <- row\n  d[row] <- ocr(date, engine = buckeye)\n  t[row] <- ocr(time, engine = buckeye)\n  \n  # move to the next file\n}\n\n# cleaning up the data\n# remove artefacts and page breaks\n\nt <- sub(\" .*\", \"\", t)\nt <- sub(\"[\\n]\", \"\", t)\n\nd <- sub(\" .*\", \"\\n\", d)\nd <- sub(\"[\\n]\", \"\", d)\n\n\n#create a dataframe with the read values\n\noutput <- data.frame(no = no,\n                     filename = files,\n                     date <- d,\n                     time <- t)\n\n#write a .csv file\n\nwrite.csv(output, \"extracted_date_and_time.csv\")\n\n```\n\nYou can now put all the photos to be tagged into the Data folder and execute the code. Depending on how many photos you have, how large your photos are and how fast your computer is it can take from minutes to hours. I was getting around 200 photos per minute on my 7 years old laptop, while my supervisor was going through more than 1000 photos per minute on his much newer and faster Mac. Still, I recommend running it in smaller chunks, that way in case of an error you are not losing 10h of processing.\n\nI intentionally didn't want to do any editing of the existing photos (disk space, maintaining 2 copies etc) and instead decided to save the results as a separate .csv file. In case you want to rename your photos and write this metadata into the picture itself, I recommend you read a very informative [post](https://johnpvanek.weebly.com/intothephrag/archives/07-2019) by John Vanek where he details how to manipulate EXIF data. Luckily, he was able to use the default tesseract engine with great success and if your camera has better resolution and uses a more standard font, you might not need to worry about training the engine at all.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Extracting date and time from camera trap photos using R and tesseract","author":"","date":"2021-03-29","slug":[],"categories":["code","R"],"tags":["R","code","camera trap","OCR"],"description":"","featured":"","featuredalt":"","featuredpath":"","linktitle":""},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}